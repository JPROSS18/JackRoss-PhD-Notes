
Expected value of self-information of a random variable. 


# Information

The Shannon information content of an outcome $x$ is defined as: 
$$
h(x) = -\log(p(x))
$$
where $x$ is a relisation of a r.v. $X$ with p.d.f $p(x)$.

# Entropy
The Entropy is defined as the average Shannon information of an outcome. 
$$
H(x) = \sum_{x \in A}p(x)h(x)
$$

Information Theory, Inference, and Learning Algorithms  - David J.C. MacKay